#!/usr/bin/env python3
# Builds EqBench programs and compares them,
# returns results how much successful was the comparison.
#
# References:
# - EqBench paper:
#   https://people.ece.ubc.ca/mjulia/publications/EQBench_2021.pdf
# - EqBench benchmarks: https://osf.io/93s5b/
# - EqBench repo: https://github.com/shrBadihi/EqBench
# - useful info from author of EqBench:
#   https://github.com/shrBadihi/EqBench/issues/2
#
# Steps to analyse DiffKemp equivalence checking using an EqBench benchmark:
# 1. download/clone the benchmark (EqBench folder)
#   from https://github.com/shrBadihi/EqBench (contains fixes)
# 2. Run this script:
#   `EqBench-workflow/run <path-to-EqBench>`
#   - it creates snapshots of programs and compares them
#   - the script informs about progress and prints result of analysis.
# 3. Detailed results of the analysis are saved in
#   <output-dir>/eqbench-results.csv (default `output-dir` is EqBench-results),
#   you can filter the result for eg. with `grep`,
#   the file is in csv format with headers
#   `type;benchmark;program;expected;result;correct;changes`
#   - `type`: type of EqBench program
#     - `aggregated`: an aggregated form of all other subfolders for a project
#     - `function-level`: for projects/programs which contains change in
#       a single function the comparison is done on function-level
#       (compared function is the one which contains changes),
#     - `program-level`: for projects/programs which contains change in
#       a single function but the comparison have to be done on program-level
#       (compared function doesn't have changes but calls function which does),
#   - `benchmark`, `program`: represents name of benchmark and program,
#   - `expected`: represents expected result of comparison (Eq/Neq)
#   - `result`: represents result of DiffKemp comparison (Eq/Neq),
#     for aggregated type if comparison was incorrect contains info
#     about how many functions were evaluated as Eq and Neq,
#   - `correct`: True if evaluation was correct, else False,

from argparse import ArgumentParser
from collections import defaultdict
from enum import Enum
import errno
from tempfile import NamedTemporaryFile
from pathlib import Path
import json
import re
import shutil
import subprocess
import sys
import time

# regex for getting info from diffkemp report stat
REGEX_TOTAL = re.compile("^Total symbols: *(\\d+)", flags=re.MULTILINE)
REGEX_EQUAL = re.compile("^Equal: *(\\d+)", flags=re.MULTILINE)
REGEX_NEQUAL = re.compile("^Not equal: *(\\d+)", flags=re.MULTILINE)

BENCHMARK_DIR = "benchmarks"
BUILD_DIR = "benchmarks-build"
COMPARE_DIR = "benchmarks-compare"

# Name of file which describes EqBench programs.
DESCRIPTION_FILE_NAME = "C-Desc.json"

# EqBench programs that do not contain a `C-Desc.json` file usually contain
# changes in all functions which are located in the program, so it is not
# necessary to specify function to compare because by default DiffKemp analysis
# all functions. However, there are some exceptions for Neq variants of
# programs where some functions remained equal and thus should not be compared.
# Manually created function list for such programs (program: function list).
AGGREGATED_FUNS_TO_COMPARE = {
    "ej_hash/ej_hash/Neq": [
        "hashCode", "testCollision1", "testCollision2", "testCollision3",
        "testCollision4"
    ],
    "raytrace/raytrace/Neq": [
        "normalize", "LightConstructor", "SphereConstructor", "intersect"
    ],
    "tcas/tcas/Neq": [
        "Non_Crossing_Biased_Descend", "Non_Crossing_Biased_Climb",
        "altseptest"
    ],
    "ell/ellProgram/Neq": [
        "snippet", "ellpi", "rf", "rd", "rj", "rc"
    ],
    "gam/gam/Neq": [
        "betai", "betacf"
    ],
    "frenel/frenelProgram/Neq": [
        "snippet", "cisi"
    ]
}


class ProgramInfo:
    """Information about EqBench program."""
    def __init__(self, src_dir):
        """:param src_dir: Path to EqBench program."""
        with open(Path(src_dir, DESCRIPTION_FILE_NAME), "r") as file:
            info = json.load(file)
        self.fun_to_compare = info["function name"]
        # For some programs (eg programs for CLEVER benchmark)
        # is necessary to do program-level analysis.
        # For these programs the name of function which should be compared
        # is probably specified after a dot in program name.
        self.program_fun = info["program name"].split(".")[1] \
            if len(info["program name"].split(".")) == 2 else None

    @staticmethod
    def exists(src_dir):
        """Checks if information about the EqBench program exists.
        If it does not exists than it is program which
        contains aggregation of all programs/changes for the benchmark.
        :param src_dir: Path to EqBench program."""
        return Path(src_dir, DESCRIPTION_FILE_NAME).exists()


class Evaluation(str, Enum):
    """
    Possible evaluation of result received by DiffKemp for certain EqBench
    program.
    """
    # True positive: non-equal program evaluated by DiffKemp as non-equal.
    TP = "TP"
    # True negative: equal program evaluated by DiffKemp as equal.
    TN = "TN"
    # False positive: equal program evaluated by DiffKemp as non-equal.
    FP = "FP"
    # False negative: non-equal program evaluated by DiffKemp as equal.
    FN = "FN"

    def __str__(self):
        return self.value


class EqBenchRun:
    """
    Class for running analysis of DiffKemp equivalence checking
    using an EqBench benchmark.
    """
    def __init__(self, args):
        self.src_dir = Path(args.source_dir, BENCHMARK_DIR)
        self.output_dir = Path(args.output_dir)
        self.build_dir = Path(args.output_dir, BUILD_DIR)
        self.cmp_dir = Path(args.output_dir, COMPARE_DIR)
        self.only_compare = args.only_compare
        # Path to diffkemp executable
        self.diffkemp = args.diffkemp
        self.disable_patterns = args.disable_patterns
        self.results_file_path = Path(self.output_dir, "eqbench-results.csv")
        self.clang_options = args.add_clang_options
        self.output_src_file_paths = args.output_src_file_paths
        self.no_opt_override = args.no_opt_override
        self.add_custom_llvm_passes = args.add_custom_llvm_passes
        self.add_cmp_opt = args.add_cmp_opt
        if self.add_cmp_opt:
            self.check_cmp_opts()
        # Commands used for building and comparing of programs created based on
        # args provided by user
        self.build_command = self._create_build_command()
        self.compare_command = self._create_compare_command()
        # Results for individual types (e.g. function-level, program-level,
        # ...) of EqBench programs.
        self.results = defaultdict(lambda: {
            Evaluation.TP: 0,
            Evaluation.TN: 0,
            Evaluation.FP: 0,
            Evaluation.FN: 0
        })
        self._run()
        self.report()

    def check_cmp_opts(self):
        """Checks if compare options specified by user exists."""
        command = [self.diffkemp, "compare", "--help"]
        output = subprocess.check_output(command, text=True)
        # todo better options checking
        for opt in self.add_cmp_opt:
            opt = opt.split("=")[0]
            if opt not in output:
                print(f"Error: '{opt}' is not option of compare command")
                sys.exit(errno.EINVAL)

    def _run(self):
        # todo parallel run to speed things up?
        if not self.only_compare:
            if self.output_dir.exists():
                shutil.rmtree(self.output_dir)
            # Creating snapshots
            print("1) Creating snapshots:", file=sys.stderr)
            print("\ta) Creating old snapshots:", file=sys.stderr)
            self._create_snapshots(new=False)
            print("\tb) Creating new snapshots:", file=sys.stderr)
            self._create_snapshots(new=True)

        # Comparing snapshots and writing results to file
        print("2) Comparing snapshots", file=sys.stderr)
        cmp_start_time = time.time()
        if self.cmp_dir.exists():
            shutil.rmtree(self.cmp_dir)
        with open(self.results_file_path, "w") as results_file:
            if self.output_src_file_paths:
                print("type;benchmark;program;expected;result;" +
                      "correct;changes;old_src;new_src",
                      file=results_file)
            else:
                print("type;benchmark;program;expected;result;correct",
                      file=results_file)
            print("\ta) Comparing Eq snapshots", file=sys.stderr)
            self._compare_snapshots(neq=False, results_file=results_file)
            print("\tb) Comparing Neq snapshots", file=sys.stderr)
            self._compare_snapshots(neq=True, results_file=results_file)
        self.cmp_runtime = time.time() - cmp_start_time
        print(f"\n\nResults are written to {str(self.results_file_path)}\n\n")

    def _create_snapshots(self, new):
        """Creates snapshots for benchmarks programs.
        :param new: If true creates snapshots of new versions of programs,
                    else of old versions.
        """
        # some files are name old.c/new.c and some oldV.c/newV.c
        # TODO count of programs specified in paper and located in dir
        # does not match
        name = "old" if not new else "new"
        files = list(Path(self.src_dir).glob(f"**/{name}*.c"))
        for file in files:
            # getting relative path to program folder
            # (benchmark/program/eq_or_neq)
            rel_path_to_file = file.relative_to(self.src_dir).parent
            print(f"\t\t{rel_path_to_file}", file=sys.stderr)
            output_path = self.build_dir / rel_path_to_file / name
            output_path.mkdir(parents=True, exist_ok=True)
            # If the program is in aggregated programs for which should be
            # compared only certain functions, build snapshot with these
            # functions.
            functions = AGGREGATED_FUNS_TO_COMPARE[str(rel_path_to_file)] \
                if str(rel_path_to_file) in AGGREGATED_FUNS_TO_COMPARE \
                else None

            self._build(str(file), str(output_path), functions)

    def _build(self, file, output_dir, functions):
        """Creates snapshot of C `file` and saves it to `output_dir`.
        :param functions: List of functions to build the snapshot for or None.
        """
        build_command = self.build_command.copy()
        build_command.extend([file, output_dir])
        # If functions are specified add them as function_list
        if functions:
            functions_file = NamedTemporaryFile()
            with open(functions_file.name, "w") as file:
                print("\n".join(functions), file=file)
            build_command.append(functions_file.name)

        subprocess.check_call(build_command, stdout=subprocess.DEVNULL)

        if self.add_custom_llvm_passes:
            llvm_file_path = str(next(Path(output_dir).glob("*.ll")))
            subprocess.check_call(["opt", llvm_file_path,
                                   f"-passes={self.add_custom_llvm_passes}",
                                   "-S", "-o", llvm_file_path])

    def _create_build_command(self):
        """Creates build command, does not include src file and output dir."""
        build_command = [
            self.diffkemp,
            "build",
        ]
        for option in self.clang_options:
            build_command.append(f"--clang-append={option.strip()}")
        if self.no_opt_override:
            build_command.append("--no-opt-override")
        return build_command

    def _compare_snapshots(self, neq, results_file):
        """Compares benchmarks snapshots of old and new programs version.
        :param neq: If true compares not equal programs,
                    else compares equal programs.
        :param results_file: FD of file to write results of evaluation.
        """
        expected_result = "Eq" if not neq else "Neq"
        build_dirs = list(Path(self.build_dir).glob(f"**/{expected_result}"))
        for build_dir in build_dirs:
            print(f"\t\t{build_dir.relative_to(self.build_dir)}",
                  file=sys.stderr)
            self._compare_snapshot(build_dir, expected_result, results_file)

    def _compare_snapshot(self, build_dir, expected_result, results_file):
        """Compares two snapshots (old, new) using DiffKemp
        and writes info about evaluation to `results_file`
        :param build_dir: Path to dir containing old/new subdirs with
                          program snapshots.
        :param expected_result: Expected result - Eq/Neq.
        :param results_file: FD to file to which to write results of eval.
        """
        old_build_dir = build_dir / "old"
        new_build_dir = build_dir / "new"

        src_dir = self.src_dir / build_dir.relative_to(self.build_dir)
        # directory containing result of comparison
        result_dir = self.cmp_dir / build_dir.relative_to(self.build_dir)
        result_dir.parent.mkdir(parents=True, exist_ok=True)

        program_info = ProgramInfo(src_dir) if ProgramInfo.exists(src_dir) \
            else None

        # If EqBench program contains info about program, the program
        # should contain change in only one function,
        # get the name of function from the information about the program
        # and compare only this function.
        # For other (aggregated) programs are compared all functions, except
        # the ones in AGGREGATED_FUNS_TO_COMPARE, for these are compared
        # only the specified functions with which the snapshots were build.
        function = None
        benchmark_type = "aggregated"
        if program_info:
            # Some functions cannot be compared on function-level but must
            # be compared on program-level to be correctly evaluated.
            if program_info.program_fun:
                benchmark_type = "program-level"
                function = program_info.program_fun
            else:
                benchmark_type = "function-level"
                function = program_info.fun_to_compare

        output = self._compare(str(old_build_dir), str(new_build_dir),
                               str(result_dir), function=function)

        result = AnalysisResult(expected_result=expected_result,
                                result_dir=result_dir,
                                output=output,
                                benchmark_type=benchmark_type)
        if self.output_src_file_paths:
            result = str(result) + \
                      f";{next(Path.glob(src_dir, 'old*.c'))}" + \
                      f";{next(Path.glob(src_dir, 'new*.c'))}"
        print(str(result), file=results_file)
        # Updating results for individual types of programs based on result
        # of current program.
        self.results[benchmark_type][result.result] += 1

    def _compare(self, old_build_dir, new_build_dir, output_dir,
                 function=None):
        """Runs `diffkemp compare` command, returns output of the command.
        :param function: Name of function to compare,
                         if it is None than it compares all functions.
        """
        compare_command = self.compare_command.copy()
        compare_command.extend([
            old_build_dir, new_build_dir,
            "-o", output_dir,
            "--report-stat",
        ])
        if function is not None:
            compare_command.extend(["--function", function])
        return subprocess.check_output(compare_command).decode("utf-8")

    def _create_compare_command(self):
        """Based on program arguments creates compare command."""
        compare_command = [
            self.diffkemp, "compare",
        ]
        if self.add_cmp_opt:
            compare_command.extend(self.add_cmp_opt)
        if self.disable_patterns:
            compare_command.extend(["--disable-all-patterns"])
        return compare_command

    def report(self):
        """Prints results of evaluation."""
        total_results = {
            Evaluation.TP: sum(
                [result[Evaluation.TP] for result in self.results.values()]),
            Evaluation.TN: sum(
                [result[Evaluation.TN] for result in self.results.values()]),
            Evaluation.FN: sum(
                [result[Evaluation.FN] for result in self.results.values()]),
            Evaluation.FP: sum(
                [result[Evaluation.FP] for result in self.results.values()]),
        }
        report = {
            "results": {
                "total": total_results,
            },
            "build-command": " ".join(self.build_command),
            "compare-command": " ".join(self.compare_command),
            "only-compare": self.only_compare,
            "compare-runtime": self.cmp_runtime,
        }
        # Adding results for individual benchmarks
        report["results"].update(self.results)
        if self.add_custom_llvm_passes:
            report["custom-llvm-passes"] = self.add_custom_llvm_passes
        # Saving summarisation of results to file
        with open(Path(self.output_dir, "result.json"), "w") as file:
            print(json.dumps(report, indent=2), file=file)
        # Print total results to stdout
        print(f"true negatives: {total_results[Evaluation.TN]}")
        print(f"false positives: {total_results[Evaluation.FP]}")
        print(f"true positives: {total_results[Evaluation.TP]}")
        print(f"false negatives: {total_results[Evaluation.FN]}")


class AnalysisResult:
    """Result of evaluation of DiffKemp comparison of EqBench program."""
    def __init__(self, expected_result, result_dir, output, benchmark_type):
        """
        :param expected_result: The expected result (Eq/Neq) of the comparison.
        :param result_dir: Path to dir containing results of the comparison.
        :param output: Stdout output of `diffkemp compare` for the program.
        :param benchmark_type: Type of benchmark "aggregate", "function-level",
                               "program-level".
        """
        self.expected_result = expected_result
        self.result_dir = result_dir
        self.total_symbols = REGEX_TOTAL.search(output).group(1)
        self.eq_symbols = REGEX_EQUAL.search(output).group(1)
        self.neq_symbols = REGEX_NEQUAL.search(output).group(1)
        self.benchmark_type = benchmark_type
        self.benchmark_name, self.program_name = \
            str(result_dir).split("/")[-3:-1]
        # Evaluating result
        if self.total_symbols == self.eq_symbols:
            info = "Eq"
        elif self.total_symbols == self.neq_symbols:
            info = "Neq"
        else:
            info = f"{self.eq_symbols} Eq, {self.neq_symbols} Neq"
        self.info = info
        # Is the program correctly evaluated?
        self.correct = True if info == self.expected_result else False
        if self.correct:
            self.result = Evaluation.TN if self.expected_result == "Eq" \
                            else Evaluation.TP
        else:
            self.result = Evaluation.FP if self.expected_result == "Eq" \
                            else Evaluation.FN

    def __str__(self):
        # `function-level` type has only one changed function in the file
        # `aggregated` contains multiple changed functions
        result_repre = [
            self.benchmark_type,
            self.benchmark_name, self.program_name,
            self.expected_result,
            self.info, str(self.correct),
        ]
        return ";".join(result_repre)


if __name__ == "__main__":
    parser = ArgumentParser(
        description="Builds EqBench programs and compares them, " +
                    "returns results how much successful was the comparison.")
    parser.add_argument("source_dir", nargs="?", default="EqBench",
                        help="EqBench source directory")
    parser.add_argument("-o", "--output-dir", default="EqBench-results",
                        help="output directory for the analysis")
    parser.add_argument("--only-compare", action="store_true",
                        help="only runs comparison, expects that " +
                        "the programs are already build " +
                        "and located in `output_dir`")
    parser.add_argument("--disable-patterns", action="store_true",
                        help="disable all built-in patterns in comparison, " +
                        "default patterns are used by default")
    parser.add_argument("--add-clang-options", action="extend", type=str,
                        default=[], nargs="+",
                        help="adds options to clang when creating snapshots" +
                        "(for options containing dashes insert space before" +
                        "or after option)")
    parser.add_argument("--output-src-file-paths", action="store_true",
                        help="saves src files paths to csv")
    parser.add_argument("--no-opt-override", action="store_true",
                        help="use optimization options provided with " +
                        "--add-clang-options")
    parser.add_argument("--add-custom-llvm-passes",
                        help="specify llvm passes to be added")
    parser.add_argument("--diffkemp", default="bin/diffkemp",
                        help="path to diffkemp executable")
    parser.add_argument("--add-cmp-opt", help="adds option to compare command",
                        action="append")
    args = parser.parse_args()
    EqBenchRun(args)
